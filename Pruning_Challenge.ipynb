{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pruning_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnppKBALvnBk",
        "colab_type": "text"
      },
      "source": [
        "#Pruning Challenge\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVbsTTb1AjJk",
        "colab_type": "text"
      },
      "source": [
        "###Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O9BhR73Ywvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import zipfile\n",
        "import glob \n",
        "\n",
        "from shutil import move,copyfile\n",
        "from numpy import linalg\n",
        "from scipy.stats import rankdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8efjlF1Jf7hs",
        "colab_type": "text"
      },
      "source": [
        "###Data Preprocessing \n",
        "In the following step I downloaded the MNIST dataset and reshaped it appropriately to obtain the training and test sets.The MNIST dataset consists of 70,000 grayscale images each 28 X 28 pixels in dimension,belonging to 10 classes and already contains a train-test split.I also normalized the pixel values and verified the final shape of the training and test matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB5pUH0AhJ0o",
        "colab_type": "code",
        "outputId": "99000c4b-9701-453c-a457-7e6a9bf28e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#loading the MNIST dataset\n",
        "(x_train,y_train), (x_test,y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "#input image dimensions\n",
        "rows, cols = 28, 28\n",
        "\n",
        "#number of output classes\n",
        "num_classes = 10\n",
        "\n",
        "#data preprocessing\n",
        "x_train = x_train.reshape(x_train.shape[0],rows*cols)\n",
        "x_test = x_test.reshape(x_test.shape[0],rows*cols)\n",
        "input_shape = (rows*cols, )\n",
        "\n",
        "#normalizing the pixel valus\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape :', x_train.shape)\n",
        "print(x_train.shape[0], 'training examples')\n",
        "print(x_test.shape[0], 'test examples')\n",
        "\n",
        "#converting class labels to binary class matrices\n",
        "y_train = tf.keras.utils.to_categorical(y_train,num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test,num_classes)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape : (60000, 784)\n",
            "60000 training examples\n",
            "10000 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o47Kz8svsY-6",
        "colab_type": "text"
      },
      "source": [
        "###Building the model\n",
        "I used Keras to build the desired neural network model as shown below.It consists of 4 hidden ReLU activated layers and a final softmax layer having 10 outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzquRl9CseFF",
        "colab_type": "code",
        "outputId": "c68a0c39-2600-42ac-d1a1-b9cdf21b404d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(1000, input_shape=input_shape),\n",
        "    tf.keras.layers.Dense(1000, activation='relu'),\n",
        "    tf.keras.layers.Dense(500, activation='relu'),\n",
        "    tf.keras.layers.Dense(200, activation='relu'),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1000)              785000    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1000)              1001000   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 500)               500500    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 200)               100200    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                2010      \n",
            "=================================================================\n",
            "Total params: 2,388,710\n",
            "Trainable params: 2,388,710\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olVDIpJUwblE",
        "colab_type": "text"
      },
      "source": [
        "###Training and Evaluation\n",
        "I trained the model for 20 epochs, using a batch size of 128.I used the **adam** optmizer and **categorical crossentropy** as the evaluation metric.\n",
        "After training, I obtained the test loss and test accuracy by evaluating the model on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XSkjigewiqE",
        "colab_type": "code",
        "outputId": "2f63ae0b-6c80-4de0-ba6d-48414c1f4c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.fit(x_train,y_train,\n",
        "          batch_size=128,\n",
        "          verbose=1,\n",
        "          epochs=20,\n",
        "          validation_data=(x_test,y_test))\n",
        "\n",
        "score = model.evaluate(x_test,y_test,verbose=0)\n",
        "\n",
        "print('Test loss :', score[0])\n",
        "print('Test accuracy :', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 0.2168 - acc: 0.9334 - val_loss: 0.1137 - val_acc: 0.9656\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.1086 - acc: 0.9660 - val_loss: 0.1030 - val_acc: 0.9694\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0839 - acc: 0.9746 - val_loss: 0.0882 - val_acc: 0.9719\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0616 - acc: 0.9815 - val_loss: 0.0929 - val_acc: 0.9720\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0638 - acc: 0.9804 - val_loss: 0.0861 - val_acc: 0.9757\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0525 - acc: 0.9833 - val_loss: 0.0948 - val_acc: 0.9733\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0451 - acc: 0.9856 - val_loss: 0.0756 - val_acc: 0.9793\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0442 - acc: 0.9864 - val_loss: 0.1013 - val_acc: 0.9765\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0375 - acc: 0.9885 - val_loss: 0.0966 - val_acc: 0.9731\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0365 - acc: 0.9890 - val_loss: 0.0843 - val_acc: 0.9788\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0341 - acc: 0.9899 - val_loss: 0.0910 - val_acc: 0.9775\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0345 - acc: 0.9900 - val_loss: 0.1138 - val_acc: 0.9732\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0317 - acc: 0.9902 - val_loss: 0.0850 - val_acc: 0.9800\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0299 - acc: 0.9917 - val_loss: 0.0886 - val_acc: 0.9790\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0262 - acc: 0.9921 - val_loss: 0.1148 - val_acc: 0.9753\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0282 - acc: 0.9914 - val_loss: 0.1149 - val_acc: 0.9767\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0256 - acc: 0.9928 - val_loss: 0.1227 - val_acc: 0.9734\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0306 - acc: 0.9911 - val_loss: 0.0908 - val_acc: 0.9790\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0245 - acc: 0.9928 - val_loss: 0.0899 - val_acc: 0.9809\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0224 - acc: 0.9937 - val_loss: 0.1055 - val_acc: 0.9778\n",
            "Test loss : 0.10550259701452523\n",
            "Test accuracy : 0.9778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob5ugDDx0sna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save model\n",
        "_, model_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model,model_file,include_optimizer=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV5Ar0RyMWD_",
        "colab_type": "text"
      },
      "source": [
        "##Weight Pruning\n",
        "To achieve sparsity of k% I ranked the individual weights in the weight matrix according to their magnitude (absolute value), and then set to zero the smallest k%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny7RhtJHNNaJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:\n",
        "  copyfile(model_file,\"/tmp/orig.h5\")\n",
        "  f=h5py.File(\"/tmp/orig.h5\",'r+')\n",
        "  ranks={}\n",
        "  for a in list(f['model_weights'])[:-1]:\n",
        "    info = f['model_weights'][a][a]['kernel:0']\n",
        "    x = np.array(info)\n",
        "    ranks[a]=(rankdata(np.abs(x),method='dense')-1).astype(int).reshape(x.shape)\n",
        "    lower_bound = np.ceil(np.max(ranks[a])*k).astype(int)\n",
        "    ranks[a][ranks[a]<=lower_bound] = 0\n",
        "    ranks[a][ranks[a]>lower_bound] = 1\n",
        "    x = x*ranks[a]\n",
        "    info[...]=x\n",
        "  f.close()\n",
        "  move(\"/tmp/orig.h5\",\"/tmp/weight_\"+str(k)+\".h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-Hd8zqmSSQc",
        "colab_type": "text"
      },
      "source": [
        "##Neuron/Unit Pruning\n",
        "To achieve sparsity of k% I ranked the columns of a weight matrix according to their L2-norm and set to zero the smallest k%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ko99joYSH_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in [.25, .50, .60, .70, .80, .90, .95, .97, .99]:\n",
        "  copyfile(model_file,\"/tmp/orig.h5\")\n",
        "  f=h5py.File(\"/tmp/orig.h5\",'r+')\n",
        "  ranks={}\n",
        "  for a in list(f['model_weights'])[:-1]:\n",
        "    info = f['model_weights'][a][a]['kernel:0']\n",
        "    x = np.array(info)\n",
        "    norm = linalg.norm(x,axis=0)\n",
        "    norm = np.tile(norm,(x.shape[0],1))\n",
        "    ranks[a]=(rankdata(norm,method='dense')-1).astype(int).reshape(norm.shape)\n",
        "    lower_bound=np.ceil(np.max(ranks[a])*k).astype(int)\n",
        "    ranks[a][ranks[a]<=lower_bound] = 0\n",
        "    ranks[a][ranks[a]>lower_bound] = 1\n",
        "    x = x*ranks[a]\n",
        "    info[...]=x\n",
        "  f.close()\n",
        "  move(\"/tmp/orig.h5\",\"/tmp/neuron\"+str(k)+\".h5\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqTTbCkXrTK9",
        "colab_type": "text"
      },
      "source": [
        "##Calculating accuracy for each value of sparsity \n",
        "For each value of k% sparsity,I calculted the accuracy and stored them in two lists,one for weight pruned models and the other for neuron/unit pruned models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v77esHxtTinK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files_weights=glob.glob('/tmp/weight*.h5')\n",
        "files_weights.sort()\n",
        "files_neurons=glob.glob('/tmp/neuron*.h5')\n",
        "files_neurons.sort()\n",
        "\n",
        "accuracy_weights=[]\n",
        "accuracy_neurons=[]\n",
        "\n",
        "for f in files_weights:\n",
        "  restored_model = tf.keras.models.load_model(f,compile=False)\n",
        "  restored_model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy'])\n",
        "  score=restored_model.evaluate(x_test,y_test,verbose=0)\n",
        "  g=h5py.File(f)\n",
        "  params=0\n",
        "  accuracy_weights.append(score[1])\n",
        "  \n",
        "for f in files_neurons:\n",
        "  restored_model = tf.keras.models.load_model(f,compile=False)\n",
        "  restored_model.compile(\n",
        "      loss=tf.keras.losses.categorical_crossentropy,\n",
        "      optimizer='adam',\n",
        "      metrics=['accuracy'])\n",
        "  score=restored_model.evaluate(x_test,y_test,verbose=0)\n",
        "  g=h5py.File(f)\n",
        "  params=0\n",
        "  accuracy_neurons.append(score[1])\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfJx1aBlruT-",
        "colab_type": "text"
      },
      "source": [
        "##Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opt2mLyJrI-S",
        "colab_type": "code",
        "outputId": "808a1d10-e830-4b31-ce67-12dec264ee7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as matpatches\n",
        "plt.rcParams['figure.constrained_layout.use']=True\n",
        "plt.xlabel('% sparsity')\n",
        "plt.ylabel('% accuracy')\n",
        "green_patch=matpatches.Patch(color='green',label='Weight Pruning')\n",
        "blue_patch=matpatches.Patch(color='blue',label='Neuron Pruning')\n",
        "plt.legend(handles=[green_patch,blue_patch],loc='upper right')\n",
        "plt.plot([0, .25, .50, .60, .70, .80, .90, .97, .99],accuracy_weights,color='green')\n",
        "plt.plot([0, .25, .50, .60, .70, .80, .90, .97, .99],accuracy_neurons,color='blue')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbgAAAEoCAYAAAAqrOTwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx3ygOqi50BN",
        "colab_type": "text"
      },
      "source": [
        "#Analysis\n",
        "\n",
        "From the above graph we can see that the acccuracy remains largely\n",
        "unchanged upto about 50% sparsity in both cases of weight as well as neuron pruning.This indicates a possibility that the baseline model is severely over-parameterized at the outset.Some of the weights(in the case of weight pruning) and neurons(in the case of neuron pruning) are redundant and capture similar information and hence their removal doesn't affect accuracy to a large extent.Also the magnitude(L1-norm in the case of weight pruning and L2-norm in the case of neuron pruning) of some of these weights or neurons is very low,and thus the changes that occur to these weights or neurons during the optimization process(i.e the gradients that are calculated during back-propagation which are used to adjust the initial weights) become insignificant as their magnitude is even lower.Thus, these weights or neurons affect the output only to a very small extent.Hence, their removal leads to only a small drop in accuracy.\n",
        "\n",
        "A large drop in accuracy is seen at about 60% sparsity in the case of neuron pruning while in the case of weight pruning accuracy is almost constant up until 80% sparsity after which a large drop in accuracy occurs.This may be due to the fact that in the case of neuron pruning,we rank the nerons in order of their L2-norm and set to zero the smallest k%.\n",
        "In some of these neurons with small L2-norms,weights capturing essential or important features might have been present, but a small L2-norm would have been obtained as a result of a majority of other weights in the neuron having low values.The probability of such a scenario increases as we increase the percentage sparsity.Weight pruning on the other hand is more targeted in the sense that it ranks individual weights and not a set of weights.Hence only those weights which are non-essential are zeroed out.Thus,a fall in accuracy occurs early(lower percentage sparsity) in the case of neuro pruning while it occurs later in the case of weight pruning.\n",
        "\n",
        "After about 90% sparsity the accuracy of both weight pruned and neuron pruned models drops to very low values as weights or neurons capturing vital information have been set to zero.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}